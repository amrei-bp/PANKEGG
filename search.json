[
  {
    "objectID": "installation_test.html",
    "href": "installation_test.html",
    "title": "Test the installation",
    "section": "",
    "text": "Once the installation is complete, you can run Pankegg using:\npankegg_make_db --help\npankegg_app --help\nIf the commands are not in your PATH then use the whole path to the executable.\nFor more detailed instructions on how to use Pankegg, see the Usage and Tests section.",
    "crumbs": [
      "Installation",
      "Test the installation"
    ]
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "This quickstart guide will walk you through setting up Pankegg, running the pipeline on test data, and launching the web application to explore your results.",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#install-pankegg-and-dependencies",
    "href": "quickstart.html#install-pankegg-and-dependencies",
    "title": "Quickstart",
    "section": "Install Pankegg and Dependencies",
    "text": "Install Pankegg and Dependencies\nInstall Pankegg via pip, conda, or pixi.\nSee the Installation section for detailed options.\nwget https://github.com/RVanDamme/PANKEGG/archive/refs/heads/master.zip\nunzip master.zip\ncd PANKEGG-master\npip install .",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#download-example-test-data-optional",
    "href": "quickstart.html#download-example-test-data-optional",
    "title": "Quickstart",
    "section": "Download Example Test Data (optional)",
    "text": "Download Example Test Data (optional)\nDownload and extract the example dataset:\nwget https://osf.io/5v3zc/download -O pankegg_test_data.zip\nunzip pankegg_test_data.zip",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#build-the-sql-database-optional",
    "href": "quickstart.html#build-the-sql-database-optional",
    "title": "Quickstart",
    "section": "Build the SQL Database (optional)",
    "text": "Build the SQL Database (optional)\nGenerate a Pankegg database from the provided CSV using the included test data (for both Sourmash and GTDB-TK classification):\npython pankegg_make_db.py -i pankegg_test_data/sourmash_example.csv -o test_sourmash --output_dir pankegg_test_data",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#launch-the-web-application",
    "href": "quickstart.html#launch-the-web-application",
    "title": "Quickstart",
    "section": "Launch the Web Application",
    "text": "Launch the Web Application\nStart the web server using the database present in the repository:\npython pankegg_app.py --d data/pankegg.db\nor use the the database you generated in point 3:\npython pankegg_app.py --d pankegg_test_data/test_sourmash",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#open-web-browser",
    "href": "quickstart.html#open-web-browser",
    "title": "Quickstart",
    "section": "Open web browser",
    "text": "Open web browser\nNow, open your web browser and go to the IP address written in your terminal ( http://0.0.0.0:5000) to interactively explore your metagenomic data!",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Pankegg",
    "section": "",
    "text": "Welcome to the Pankegg documentation. Here you will find all the information you need to get started with Pankegg.",
    "crumbs": [
      "Home",
      "Welcome to Pankegg"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to Pankegg",
    "section": "Overview",
    "text": "Overview\nA powerful parser and visualizer for metagenome bin annotation, classification, and quality assessment.\nPankegg is a flexible and user-friendly interactive software suite built with Python, SQL, Jinja, JavaScript, and Flask, designed to visualize and compare bin annotations, taxonomic classifications, and quality metrics (estimated by CheckM2). It enables interactive exploration and comparison of results across bins and samples through a local web server interface.\nPankegg is ideal for anyone working with output files from CheckM2, EggNOG, Sourmash, or GTDB-TK. While it can be integrated into any workflow, it was originally conceived as the final step of the MUFFIN pipeline to provide a comprehensive visualization and analysis platform.",
    "crumbs": [
      "Home",
      "Welcome to Pankegg"
    ]
  },
  {
    "objectID": "index.html#mission-statement",
    "href": "index.html#mission-statement",
    "title": "Welcome to Pankegg",
    "section": "Mission Statement",
    "text": "Mission Statement\nPankegg aims to provide a comprehensive tool for the analysis and visualization of metabolic pathways across various samples and bins.\nOur goal is to facilitate research and understanding in the field of metagenomics and bioinformatics.",
    "crumbs": [
      "Home",
      "Welcome to Pankegg"
    ]
  },
  {
    "objectID": "index.html#features-and-benefits",
    "href": "index.html#features-and-benefits",
    "title": "Welcome to Pankegg",
    "section": "Features and Benefits",
    "text": "Features and Benefits\n\nCompare samples and bins with ease\nVisualize taxonomic compositions\nAnalyze pathway completions\nExport data for further analysis",
    "crumbs": [
      "Home",
      "Welcome to Pankegg"
    ]
  },
  {
    "objectID": "installation.html#install-pankegg",
    "href": "installation.html#install-pankegg",
    "title": "Download and Install",
    "section": "Install PANKEGG",
    "text": "Install PANKEGG\nInstalling Pankegg is simple. You can either download the repository as a zip file, or clone it using git.\n\nDownload via wget or curl\nYou can download the repository as a zip file with wget:\nwget https://github.com/RVanDamme/PANKEGG/archive/refs/heads/master.zip\nunzip master.zip\ncd PANKEGG-master\nOr with curl:\ncurl -L https://github.com/RVanDamme/PANKEGG/archive/refs/heads/master.zip -o master.zip\nunzip master.zip\ncd PANKEGG-master\nOR\n\n\nClone via git\nAlternatively, you can clone the repository directly using git:\ngit clone https://github.com/RVanDamme/PANKEGG.git\ncd PANKEGG",
    "crumbs": [
      "Installation",
      "Download and Install"
    ]
  },
  {
    "objectID": "installation.html#install-dependencies",
    "href": "installation.html#install-dependencies",
    "title": "Download and Install",
    "section": "Install dependencies",
    "text": "Install dependencies\nYou can install Pankegg and all necessary dependencies via conda, pip, or pixi.\n\nUsing conda\nFor a stable installation, use conda-lock to create and activate a stable conda environment using the provided conda-lock.yml file (found in the conda.recipe directory):\nconda-lock install --name pankegg_env conda-lock.yml\nconda activate pankegg_env\nAlternatively, create and activate a new conda environment using the provided environment.yml file (found in the conda.recipe directory):\nconda env create -f environment.yml\nconda activate pankegg_env\n\n\nUsing pip\nFrom within the repository folder, install all dependencies by running:\npip install .\nOr, install dependencies only:\npip install flask pandas numpy scikit-learn scipy jinja2 click setuptools importlib-metadata\n\n\nUsing pixi\nIf you use pixi, you can use the pixi.lock and run all the Pankegg commands with:\npixi run &lt;pankegg_command&gt;\nSubstitube &lt;pankegg_command&gt; with the command you want to run.",
    "crumbs": [
      "Installation",
      "Download and Install"
    ]
  },
  {
    "objectID": "installation.html#windows-subsystem-for-linux",
    "href": "installation.html#windows-subsystem-for-linux",
    "title": "Download and Install",
    "section": "Windows Subsystem for Linux",
    "text": "Windows Subsystem for Linux\nIf you are using WSL, you should install Pankegg in the WSL itself and NOT on your Windows drive. This is required because of the disparity between the Linux FileSystem and the Windows FileSystem, which prevents the tools from running correctly through WSL when installed in the Windows FS.\nWe also recommend storing your database in the WSL rather than on your Windows drive. While this is not critical, it is a better practice and ensures that Pankegg will read the database properly.",
    "crumbs": [
      "Installation",
      "Download and Install"
    ]
  },
  {
    "objectID": "authors.html#ai-and-llm",
    "href": "authors.html#ai-and-llm",
    "title": "Authors and contributors",
    "section": "AI and LLM",
    "text": "AI and LLM\nAI/LLM were used as “writing buddy” during the redaction of the README.md (e.g. ensuring a readable markdown structure and correcting the syntax/grammar). The logic and code were initially made by humans (@RVanDamme and @Avanbelle), before using AI/LLM on the code to debug and refactor it.",
    "crumbs": [
      "Additional Information",
      "Authors and contributors"
    ]
  },
  {
    "objectID": "Pankegg_make_db.html",
    "href": "Pankegg_make_db.html",
    "title": "Pankegg_make_db.py",
    "section": "",
    "text": "Pankegg’s data parser that constructs the SQL database that is later used in the web application.",
    "crumbs": [
      "The tools",
      "Pankegg_make_db.py"
    ]
  },
  {
    "objectID": "Pankegg_make_db.html#input-file-format",
    "href": "Pankegg_make_db.html#input-file-format",
    "title": "Pankegg_make_db.py",
    "section": "Input file format",
    "text": "Input file format\nTo create the SQL database, you will need a CSV file listing all your samples and their corresponding input directories or files.\nThe expected CSV format uses the following header:\nSample name,Annotation_dir,classification_dir,Checkm2_dir\nEach subsequent line represents a sample, and should look like one of the following examples:\n\nFor Sourmash classification:\n\n    SAMPLE1,/Path/to/SAMPLE1/bin_annotation/*.annotations.tsv,/Path/to/SAMPLE1/sourmash/*,/Path/to/SAMPLE1/checkm2_dir/quality_report.tsv\n    SAMPLE2,/Path/to/SAMPLE2/bin_annotation/*.annotations.tsv,/Path/to/SAMPLE2/sourmash/*,/Path/to/SAMPLE2/checkm2_dir/quality_report.tsv\n\nFor GTDB-TK classification (when using the --gtdbtk flag):\n\n    SAMPLE1,/Path/to/SAMPLE1/bin_annotation/*.annotations.tsv,/Path/to/SAMPLE1/gtdb_results/*.summary.tsv,/Path/to/SAMPLE1/checkm2_dir/quality_report.tsv\n    SAMPLE2,/Path/to/SAMPLE2/bin_annotation/*.annotations.tsv,/Path/to/SAMPLE2/gtdb_results/*.summary.tsv,/Path/to/SAMPLE2/checkm2_dir/quality_report.tsv\n\nCreate the input file\nYou can create this input CSV file automatically with a bash loop, depending on how your data are organized:\n\nIf your results are grouped by sample:\nDir/\n├── SAMPLEID1/\n│   ├── bin_annotation/\n│   ├── sourmash/\n│   ├── gtdb_results/\n│   └── checkm2_dir/\n└── SAMPLEID2/\n    ├── bin_annotation/\n    ├── sourmash/\n    ├── gtdb_results/\n    └── checkm2_dir/\nRun the following script to include Sourmash annotation:\necho \"Sample name,Annotation_dir,classification_dir,Checkm2_dir\" &gt; samples.csv\nfor sample in Dir/*; do\n    name=$(basename \"$sample\");\n    ann_path=\"$sample/bin_annotation/*.annotations.tsv\";\n    class_path=\"$sample/sourmash/*\";\n    checkm2_path=\"$sample/checkm2_dir/quality_report.tsv\";\n    echo \"$name,$ann_path,$class_path,$checkm2_path\" &gt;&gt; samples.csv;\ndone\nRun the following script to include GTDBTK annotation:\necho \"Sample name,Annotation_dir,classification_dir,Checkm2_dir\" &gt; samples.csv\nfor sample in Dir/*; do\n   name=$(basename \"$sample\");\n   ann_path=\"$sample/bin_annotation/*.annotations.tsv\";\n   class_path=\"$sample/gtdb_results/*.summary.tsv\";\n   checkm2_path=\"$sample/checkm2_dir/quality_report.tsv\";\n   echo \"$name,$ann_path,$class_path,$checkm2_path\" &gt;&gt; samples.csv;\ndone\n\n\nIf your results are grouped by tool, then sample:\nDir/\n├── bin_annotation/\n│   ├── SAMPLEID1/\n│   ├── SAMPLEID2/\n│   └── ...\n├── sourmash/\n│   ├── SAMPLEID1/\n│   ├── SAMPLEID2/\n│   └── ...\n├── gtdb_results/\n│   ├── SAMPLEID1/\n│   ├── SAMPLEID2/\n│   └── ...\n└── checkm2_dir/\n   ├── SAMPLEID1/\n   ├── SAMPLEID2/\n   └── ...\nRun the following script to include Sourmash annotation:\necho \"Sample name,Annotation_dir,classification_dir,Checkm2_dir\" &gt; samples.csv\nfor sample in Dir/bin_annotation/*; do\n    name=$(basename \"$sample\");\n    ann_path=\"Dir/bin_annotation/$name/*.annotations.tsv\";\n    class_path=\"Dir/sourmash/$name/*\";\n    checkm2_path=\"Dir/checkm2_dir/$name/quality_report.tsv\";\n    echo \"$name,$ann_path,$class_path,$checkm2_path\" &gt;&gt; samples.csv;\ndone\nRun the following script to include GTDBTK annotation:\necho \"Sample name,Annotation_dir,classification_dir,Checkm2_dir\" &gt; samples.csv\nfor sample in Dir/bin_annotation/*; do\n    name=$(basename \"$sample\");\n    ann_path=\"Dir/bin_annotation/$name/*.annotations.tsv\";\n    class_path=\"Dir/gtdb_results/$name/*.summary.tsv\";\n    checkm2_path=\"Dir/checkm2_dir/$name/quality_report.tsv\";\n    echo \"$name,$ann_path,$class_path,$checkm2_path\" &gt;&gt; samples.csv;\ndone",
    "crumbs": [
      "The tools",
      "Pankegg_make_db.py"
    ]
  },
  {
    "objectID": "Pankegg_make_db.html#parameters-make-db",
    "href": "Pankegg_make_db.html#parameters-make-db",
    "title": "Pankegg_make_db.py",
    "section": "Parameters Make DB",
    "text": "Parameters Make DB\n\n-i, --input\nPath to the CSV file listing all samples and their input files/directories (required).\n-o, --output\nName of the output database file (without the extension). The default is pankegg. The .db extension is automatically added to the output name.\n--output_dir\nDirectory the database will be written to. The default is ./db_output.\n--gtdbtk\nUse this flag if your classification files were generated with GTDB-TK instead of Sourmash.",
    "crumbs": [
      "The tools",
      "Pankegg_make_db.py"
    ]
  },
  {
    "objectID": "Pankegg_make_db.html#output-make-db",
    "href": "Pankegg_make_db.html#output-make-db",
    "title": "Pankegg_make_db.py",
    "section": "Output Make DB",
    "text": "Output Make DB\nThe output is an SQLite database (*.db) that can be opened with tools like sqlite3, but is best used with pankegg_app.py for interactive browsing.\nThe database contains the following tables:\n\ntaxonomy\nbin\nmap\nkegg\nbin_map_kegg\nbin_map\nmap_kegg\nbin_extra\nbin_extra_kegg\nsample\n\nEach table stores specific information related to bins, pathways, taxonomy, and annotation results for easy querying and visualization.",
    "crumbs": [
      "The tools",
      "Pankegg_make_db.py"
    ]
  },
  {
    "objectID": "Pankegg_make_db.html#run-with-testdata",
    "href": "Pankegg_make_db.html#run-with-testdata",
    "title": "Pankegg_make_db.py",
    "section": "Run with Testdata",
    "text": "Run with Testdata\nTo verify your installation and familiarize yourself with Pankegg, you can run a test using provided data. Download the example archive, unzip it, and generate test databases using the included CSV files:\n\nDownload the test data archive from OSF:\nwget https://osf.io/download/5v3zc/ -O pankegg_test_data.zip\nOR\ncurl -L -o pankegg_test_data.zip  https://osf.io/download/5v3zc/\n\n\nUnzip the archive:\nThis will create a directory called pankegg_test_data.\nunzip pankegg_test_data.zip\n\n\nCreate a test database for Sourmash classification:\npython pankegg_make_db.py -i pankegg_test_data/sourmash_example.csv -o test_sourmash --output_dir pankegg_test_data\n\n\nCreate a test database for GTDB-TK classification:\npython pankegg_make_db.py -i pankegg_test_data/gtdbtk_example.csv -o test_gtdbtk --output_dir pankegg_test_data --gtdbtk\nAfter running these commands, you should find newly created test_sourmash.db and test_gtdbtk.db inside the pankegg_test_data directory. This is in addition to the already existing sourmash_example.db and gtdbtk_example.db files in the same directory.\nThe existing and newly generated respective databases should be identical, so the validity of pankegg_make_db.py can be tested by comparing the two files.",
    "crumbs": [
      "The tools",
      "Pankegg_make_db.py"
    ]
  },
  {
    "objectID": "Pankegg_make_db.html#troubleshooting",
    "href": "Pankegg_make_db.html#troubleshooting",
    "title": "Pankegg_make_db.py",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nFor more details or troubleshooting, please consult the Reporting Bugs & Contributing section.",
    "crumbs": [
      "The tools",
      "Pankegg_make_db.py"
    ]
  },
  {
    "objectID": "input_files.html",
    "href": "input_files.html",
    "title": "Supported input",
    "section": "",
    "text": "Pankegg was developed to integrate output files from a variety of metagenomics tools, such as CheckM2, Sourmash, GTDB-TK and EggNOG. You can combine them all, or only add the ones that are interesting for you.",
    "crumbs": [
      "Getting Started",
      "Supported input"
    ]
  },
  {
    "objectID": "input_files.html#input-files",
    "href": "input_files.html#input-files",
    "title": "Supported input",
    "section": "Input files",
    "text": "Input files\n\nSupported input files\n\n\n\n\n\n\n\nTool\nInput File\nData Extracted\n\n\n\n\ncheckm2\nquality_report.tsv\nbin completeness and contamination\n\n\nSourmash\n*.txt\nID, Status and Taxonomic classification\n\n\nGTDB-TK\n*.summary.tsv\nUser genome and Taxonomic classification\n\n\nEggNOG\n*.annotations.tsv\nKEGG Orthologs, KEGG Pathways, GO terms, Description",
    "crumbs": [
      "Getting Started",
      "Supported input"
    ]
  },
  {
    "objectID": "input_files.html#additional-input-files-provided-by-pankegg",
    "href": "input_files.html#additional-input-files-provided-by-pankegg",
    "title": "Supported input",
    "section": "Additional Input Files Provided by Pankegg",
    "text": "Additional Input Files Provided by Pankegg\nPANKEGG requires more input files. However, these are already provided and you don’t need to worry about them. They are located in the data directory.\n\nAdditional input, built into PANKEGG\n\n\n\n\n\n\n\nFile Name\nDescription\nData Origin\n\n\n\n\nkegg_map_info.tsv\nContains mapID, pathway name, and total number of orthologs per pathway (map)\nExtracted from KEGG pathway database\n\n\nkegg_map_orthologs.tsv\nContains mapID, pathway name, and list of orthologs per pathway (map)\nExtracted from KEGG pathway database\n\n\nko.txt\nContains the IDs and names of orthologs\nExtracted from KEGG orthologs database\n\n\npathway.txt\nContains map IDs (pathways) and their names\nExtracted from KEGG pathway database\n\n\npathway_groups.txt\nMaps IDs grouped by KEGG sub-categories\nExtracted from KEGG pathway database",
    "crumbs": [
      "Getting Started",
      "Supported input"
    ]
  },
  {
    "objectID": "tool_introduction.html",
    "href": "tool_introduction.html",
    "title": "Introduction to the tools",
    "section": "",
    "text": "Pankegg is composed of two main tools:\nThe web interface is divided into two main categories:",
    "crumbs": [
      "The tools",
      "Introduction to the tools"
    ]
  },
  {
    "objectID": "tool_introduction.html#feature-pages",
    "href": "tool_introduction.html#feature-pages",
    "title": "Introduction to the tools",
    "section": "Feature Pages",
    "text": "Feature Pages\nPankegg provides multiple interactive features:\n\nSample vs Sample: Heatmap of selected pathways, scatter plots for bin quality, PCA of bins, tables/plots showing KEGGs unique or shared between samples/pathways.\nBin vs Bin: Plot and table of KEGGs shared or unique between two bins for metabolic pathways.\nTaxonomic Comparison: For a given rank, see sample-wise composition plots (abundance = number of bins classified as taxon / total bins), and pathway-vs-taxa heatmaps.\nPCA: Dedicated page for visualizing principal component analysis (PCA) of each sample, based on KOs, maps, or taxonomy.\n\n\nNote:\nIn this documentation, the terms “map” and “pathway” are sometimes used interchangeably. Typically, “map” refers to the KEGG database’s map ID (e.g., map00010), while “pathway” refers to the biological pathway name. Although the KEGG database provides both a pathway ID and a map ID, this tool focuses on the map ID, which is generally more complete and reliable for referencing metabolic pathways.",
    "crumbs": [
      "The tools",
      "Introduction to the tools"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Reporting bugs & contributing",
    "section": "",
    "text": "We thank you for taking the time to contribute, whether with bugs, code or with ideas, to the project.\n\n\nBefore contributing, take a moment to explore the repository:\n\npankegg_make_db.py: builds the SQLite database from your annotation CSV files.\npankegg_app.py: launches the Flask web interface to browse bins and KEGG pathways.\nlib/: utility modules with SQL commands and database helpers.\ntemplates/: Jinja HTML templates for the web pages.\ndata/: example files and a sample pankegg.db.\nsetup.py and .github/workflows/: packaging and CI configuration.\n\nWe recommend inspecting data/pankegg.db with sqlite3 and reviewing the templates to see how queries render in the app. The README walks through building a database and starting the server.\n\n\n\n\nEnsure that the bug was not already reported by searching reported Issues.\nIf you’re unable to find an (open) issue addressing the problem, open a new one. Be sure to prefix the issue title with [BUG] and to include:\na clear description,\nas much relevant information as possible, and\na code sample or a database test case demonstrating the problematic behaviour that is occurring.\n\n\n\n\nCreate an issue on Github or you can alternatively pick one already created.\nAssign yourself to that issue.\nDiscussions on how to proceed about that issue take place in the comment section on that issue.\nSome of the work might have been done already by somebody, hence we avoid unnecessary work duplication and a waste of time and effort. Other reason for discussing the issue beforehand is to communicate with the team the changes as some of the features might impact different components, and we can plan accordingly.\n\n\n\nAll work should take place in a dedicated branch with a short descriptive name.\nUse comments in your code, choose variable and function names that clearly show what you intend to implement.\nOnce the feature is done you can request it to be merged back into main by making a Pull Request.\nBefore making the pull request it is a good idea to rebase your branch to main to ensure that eventual conflicts with the main branch is solved before the PR is reviewed and we can therefore have a clean merge.\n\n\nIn general it is better to commit often. Small commits are easier to roll back and also makes the code easier to review.\nWrite helpful commit messages that describes the changes and possibly why they were necessary.\nEach commit should contain changes that are functionally connected and/or related. If you for example want to write and in the first line of the commit message this is an indicator that it should have been two commits.\nLearn how to select chunks of changed files to do multiple separate commits of unrelated things. This can be done with either git add -p or git commit -p.\n\n\nThe commit messages may be seen as meta-comments on the code that are incredibly helpful for anyone who wants to know how this piece of software is working, including colleagues (current and future) and external users.\nSome tips about writing helpful commit messages:\n\nSeparate subject (the first line of the message) from body with a blank line.\nLimit the subject line to 50 characters.\nCapitalize the subject line.\nDo not end the subject line with a period.\nUse the imperative mood in the subject line.\nWrap the body at 72 characters.\nUse the body to explain what and why vs. how.\n\nFor an in-depth explanation of the above points, please see How to Write a Git Commit Message.\n\n\n\n\nA code review is initiated when someone has made a Pull Request in the appropriate repo on github.\nWork should not continue on the branch unless it is a Draft Pull Request. Once the PR is marked ready the review can start.\nThe initiator of the PR should recruit a reviewer that get assigned reviewer duty on the branch.\nOther people may also look at and review the code.\nA reviewers job is to:\n\nWrite polite and friendly comments - remember that it can be tough to have other people critizising your work, a little kindness goes a long way. This does not mean we should not comment on things that need to be changed of course.\nRead the code and make sure it is understandable\nMake sure that commit messages and commits are structured so that it is possible to understand why certain changes were made.\n\nOnce the review is positive the Pull Request can be merged into main and the feature branch deleted.\n\nThanks again.",
    "crumbs": [
      "Additional Information",
      "Reporting bugs & contributing"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#getting-started-with-pankegg",
    "href": "CONTRIBUTING.html#getting-started-with-pankegg",
    "title": "Reporting bugs & contributing",
    "section": "",
    "text": "Before contributing, take a moment to explore the repository:\n\npankegg_make_db.py: builds the SQLite database from your annotation CSV files.\npankegg_app.py: launches the Flask web interface to browse bins and KEGG pathways.\nlib/: utility modules with SQL commands and database helpers.\ntemplates/: Jinja HTML templates for the web pages.\ndata/: example files and a sample pankegg.db.\nsetup.py and .github/workflows/: packaging and CI configuration.\n\nWe recommend inspecting data/pankegg.db with sqlite3 and reviewing the templates to see how queries render in the app. The README walks through building a database and starting the server.",
    "crumbs": [
      "Additional Information",
      "Reporting bugs & contributing"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#did-you-find-a-bug",
    "href": "CONTRIBUTING.html#did-you-find-a-bug",
    "title": "Reporting bugs & contributing",
    "section": "",
    "text": "Ensure that the bug was not already reported by searching reported Issues.\nIf you’re unable to find an (open) issue addressing the problem, open a new one. Be sure to prefix the issue title with [BUG] and to include:\na clear description,\nas much relevant information as possible, and\na code sample or a database test case demonstrating the problematic behaviour that is occurring.",
    "crumbs": [
      "Additional Information",
      "Reporting bugs & contributing"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-work-on-a-new-featurebug",
    "href": "CONTRIBUTING.html#how-to-work-on-a-new-featurebug",
    "title": "Reporting bugs & contributing",
    "section": "",
    "text": "Create an issue on Github or you can alternatively pick one already created.\nAssign yourself to that issue.\nDiscussions on how to proceed about that issue take place in the comment section on that issue.\nSome of the work might have been done already by somebody, hence we avoid unnecessary work duplication and a waste of time and effort. Other reason for discussing the issue beforehand is to communicate with the team the changes as some of the features might impact different components, and we can plan accordingly.",
    "crumbs": [
      "Additional Information",
      "Reporting bugs & contributing"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#how-we-work-with-git",
    "href": "CONTRIBUTING.html#how-we-work-with-git",
    "title": "Reporting bugs & contributing",
    "section": "",
    "text": "All work should take place in a dedicated branch with a short descriptive name.\nUse comments in your code, choose variable and function names that clearly show what you intend to implement.\nOnce the feature is done you can request it to be merged back into main by making a Pull Request.\nBefore making the pull request it is a good idea to rebase your branch to main to ensure that eventual conflicts with the main branch is solved before the PR is reviewed and we can therefore have a clean merge.\n\n\nIn general it is better to commit often. Small commits are easier to roll back and also makes the code easier to review.\nWrite helpful commit messages that describes the changes and possibly why they were necessary.\nEach commit should contain changes that are functionally connected and/or related. If you for example want to write and in the first line of the commit message this is an indicator that it should have been two commits.\nLearn how to select chunks of changed files to do multiple separate commits of unrelated things. This can be done with either git add -p or git commit -p.\n\n\nThe commit messages may be seen as meta-comments on the code that are incredibly helpful for anyone who wants to know how this piece of software is working, including colleagues (current and future) and external users.\nSome tips about writing helpful commit messages:\n\nSeparate subject (the first line of the message) from body with a blank line.\nLimit the subject line to 50 characters.\nCapitalize the subject line.\nDo not end the subject line with a period.\nUse the imperative mood in the subject line.\nWrap the body at 72 characters.\nUse the body to explain what and why vs. how.\n\nFor an in-depth explanation of the above points, please see How to Write a Git Commit Message.\n\n\n\n\nA code review is initiated when someone has made a Pull Request in the appropriate repo on github.\nWork should not continue on the branch unless it is a Draft Pull Request. Once the PR is marked ready the review can start.\nThe initiator of the PR should recruit a reviewer that get assigned reviewer duty on the branch.\nOther people may also look at and review the code.\nA reviewers job is to:\n\nWrite polite and friendly comments - remember that it can be tough to have other people critizising your work, a little kindness goes a long way. This does not mean we should not comment on things that need to be changed of course.\nRead the code and make sure it is understandable\nMake sure that commit messages and commits are structured so that it is possible to understand why certain changes were made.\n\nOnce the review is positive the Pull Request can be merged into main and the feature branch deleted.\n\nThanks again.",
    "crumbs": [
      "Additional Information",
      "Reporting bugs & contributing"
    ]
  },
  {
    "objectID": "Pankegg_app.html",
    "href": "Pankegg_app.html",
    "title": "Pankegg_app.py",
    "section": "",
    "text": "Pankegg’s web server for interactive data exploration.",
    "crumbs": [
      "The tools",
      "Pankegg_app.py"
    ]
  },
  {
    "objectID": "Pankegg_app.html#input-file",
    "href": "Pankegg_app.html#input-file",
    "title": "Pankegg_app.py",
    "section": "Input file",
    "text": "Input file\nPankegg_app.py is using the output of pankegg_make_db.py.",
    "crumbs": [
      "The tools",
      "Pankegg_app.py"
    ]
  },
  {
    "objectID": "Pankegg_app.html#usage-app",
    "href": "Pankegg_app.html#usage-app",
    "title": "Pankegg_app.py",
    "section": "Usage APP",
    "text": "Usage APP\nTo start the Pankegg web server with your database, use the following command (replace with your actual database path):\npython /path/to/pankegg_app.py --d /Path/to/YOUR_PANKEGG.db\n(Replace /path/to/pankegg_app.py and /Path/to/YOUR_PANKEGG.db with your actual paths.)\n\nNote:\nBefore running the web application, it is highly recommended to change the app.secret_key (located around line 24 in pankegg_app.py) to a secure value of your choice. This key is used by Flask for session security. Leaving it as the default (‘local’) is fine for local or testing use, but for any real-world or multi-user deployment, you should generate and use a strong, unique secret key to prevent session tampering and increase security.",
    "crumbs": [
      "The tools",
      "Pankegg_app.py"
    ]
  },
  {
    "objectID": "Pankegg_app.html#parameters-app",
    "href": "Pankegg_app.html#parameters-app",
    "title": "Pankegg_app.py",
    "section": "Parameters APP",
    "text": "Parameters APP\n\n--d or --db\nPath to the SQLite database you want to browse.\nExample: --d /Path/to/YOUR_PANKEGG.db\n--help\nDisplay a help message with all available options.",
    "crumbs": [
      "The tools",
      "Pankegg_app.py"
    ]
  },
  {
    "objectID": "Pankegg_app.html#output-app",
    "href": "Pankegg_app.html#output-app",
    "title": "Pankegg_app.py",
    "section": "Output APP",
    "text": "Output APP\nBy default, the web server will start on your local IP address (host 0.0.0.0) and port 5000.\nOnce started, you can access the Pankegg interface in your browser by navigating to:\nhttp://0.0.0.0:5000\nYou will be able to browse, filter, and visualize all results contained in your selected database.",
    "crumbs": [
      "The tools",
      "Pankegg_app.py"
    ]
  },
  {
    "objectID": "Pankegg_app.html#run-with-testdata",
    "href": "Pankegg_app.html#run-with-testdata",
    "title": "Pankegg_app.py",
    "section": "Run with Testdata",
    "text": "Run with Testdata\nYou can test the web interface using either the database provided with the repository, example databases from the test data, or your own generated databases.\nHere are example commands (replace BASH with the actual commands):\n\nTo run the app on the database shipped with the GitHub repository (data/pankegg.db):\n\npython pankegg_app.py --d data/pankegg.db\n\nTo run the app on the example Sourmash test database (pankegg_test_data/sourmash_example.db):\n\npython pankegg_app.py --d pankegg_test_data/sourmash_example.db\n\nTo run the app on the example GTDB-TK test database (pankegg_test_data/gtdbtk_example.db):\n\npython pankegg_app.py --d pankegg_test_data/gtdbtk_example.db\n\nTo run the app on the Sourmash database you created during the test (pankegg_test_data/test_sourmash.db):\n\npython pankegg_app.py --d pankegg_test_data/test_sourmash.db\n\nTo run the app on the GTDB-TK database you created during the test (pankegg_test_data/test_gtdbtk.db):\n\npython pankegg_app.py --d pankegg_test_data/test_gtdbtk.db",
    "crumbs": [
      "The tools",
      "Pankegg_app.py"
    ]
  },
  {
    "objectID": "Pankegg_app.html#troubleshooting",
    "href": "Pankegg_app.html#troubleshooting",
    "title": "Pankegg_app.py",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nFor more details or troubleshooting, please consult the Reporting Bugs & Contributing section.",
    "crumbs": [
      "The tools",
      "Pankegg_app.py"
    ]
  },
  {
    "objectID": "reporting_bugs.html#getting-started-with-pankegg",
    "href": "reporting_bugs.html#getting-started-with-pankegg",
    "title": "Reporting bugs & contributing",
    "section": "Getting started with Pankegg",
    "text": "Getting started with Pankegg\nBefore contributing, take a moment to explore the repository:\n\npankegg_make_db.py: builds the SQLite database from your annotation CSV files.\npankegg_app.py: launches the Flask web interface to browse bins and KEGG pathways.\nlib/: utility modules with SQL commands and database helpers.\ntemplates/: Jinja HTML templates for the web pages.\ndata/: example files and a sample pankegg.db.\nsetup.py and .github/workflows/: packaging and CI configuration.\n\nWe recommend inspecting data/pankegg.db with sqlite3 and reviewing the templates to see how queries render in the app. The README walks through building a database and starting the server."
  },
  {
    "objectID": "reporting_bugs.html#did-you-find-a-bug",
    "href": "reporting_bugs.html#did-you-find-a-bug",
    "title": "Reporting bugs & contributing",
    "section": "Did you find a bug?",
    "text": "Did you find a bug?\n\nEnsure that the bug was not already reported by searching reported Issues.\nIf you’re unable to find an (open) issue addressing the problem, open a new one. Be sure to prefix the issue title with [BUG] and to include:\na clear description,\nas much relevant information as possible, and\na code sample or a database test case demonstrating the problematic behaviour that is occurring."
  },
  {
    "objectID": "reporting_bugs.html#how-to-work-on-a-new-featurebug",
    "href": "reporting_bugs.html#how-to-work-on-a-new-featurebug",
    "title": "Reporting bugs & contributing",
    "section": "How to work on a new feature/bug",
    "text": "How to work on a new feature/bug\nCreate an issue on Github or you can alternatively pick one already created.\nAssign yourself to that issue.\nDiscussions on how to proceed about that issue take place in the comment section on that issue.\nSome of the work might have been done already by somebody, hence we avoid unnecessary work duplication and a waste of time and effort. Other reason for discussing the issue beforehand is to communicate with the team the changes as some of the features might impact different components, and we can plan accordingly."
  },
  {
    "objectID": "reporting_bugs.html#how-we-work-with-git",
    "href": "reporting_bugs.html#how-we-work-with-git",
    "title": "Reporting bugs & contributing",
    "section": "How we work with Git",
    "text": "How we work with Git\nAll work should take place in a dedicated branch with a short descriptive name.\nUse comments in your code, choose variable and function names that clearly show what you intend to implement.\nOnce the feature is done you can request it to be merged back into main by making a Pull Request.\nBefore making the pull request it is a good idea to rebase your branch to main to ensure that eventual conflicts with the main branch is solved before the PR is reviewed and we can therefore have a clean merge.\n\nGeneral stuff about git and commit messages\nIn general it is better to commit often. Small commits are easier to roll back and also makes the code easier to review.\nWrite helpful commit messages that describes the changes and possibly why they were necessary.\nEach commit should contain changes that are functionally connected and/or related. If you for example want to write and in the first line of the commit message this is an indicator that it should have been two commits.\nLearn how to select chunks of changed files to do multiple separate commits of unrelated things. This can be done with either git add -p or git commit -p.\n\nHelpful commit messages\nThe commit messages may be seen as meta-comments on the code that are incredibly helpful for anyone who wants to know how this piece of software is working, including colleagues (current and future) and external users.\nSome tips about writing helpful commit messages:\n\nSeparate subject (the first line of the message) from body with a blank line.\nLimit the subject line to 50 characters.\nCapitalize the subject line.\nDo not end the subject line with a period.\nUse the imperative mood in the subject line.\nWrap the body at 72 characters.\nUse the body to explain what and why vs. how.\n\nFor an in-depth explanation of the above points, please see How to Write a Git Commit Message.\n\n\n\nHow we do code reviews\nA code review is initiated when someone has made a Pull Request in the appropriate repo on github.\nWork should not continue on the branch unless it is a Draft Pull Request. Once the PR is marked ready the review can start.\nThe initiator of the PR should recruit a reviewer that get assigned reviewer duty on the branch.\nOther people may also look at and review the code.\nA reviewers job is to:\n\nWrite polite and friendly comments - remember that it can be tough to have other people critizising your work, a little kindness goes a long way. This does not mean we should not comment on things that need to be changed of course.\nRead the code and make sure it is understandable\nMake sure that commit messages and commits are structured so that it is possible to understand why certain changes were made.\n\nOnce the review is positive the Pull Request can be merged into main and the feature branch deleted.\n\nThanks again."
  },
  {
    "objectID": "web_interface.html",
    "href": "web_interface.html",
    "title": "The Pankegg Web Page",
    "section": "",
    "text": "This is an overview of all the web app pages:",
    "crumbs": [
      "Web Interface",
      "The Pankegg Web Page"
    ]
  },
  {
    "objectID": "web_interface.html#home",
    "href": "web_interface.html#home",
    "title": "The Pankegg Web Page",
    "section": "Home",
    "text": "Home\nAt the top, a navigation bar provides quick access to all major sections: Home, Bin, Map, Kegg, Taxonomy, and Visualisation. Each of these sections links to a page where you can browse detailed information about your bins, pathway maps, KEGG identifiers, and taxonomic classifications, or use the different interactive visualisation tools. The navbar ensures that you can switch between functional modules.\nThe landing page, Home, links to all elements that are also present in the navigation bar, and provides a short description for each one. Additionally, it shows the full content of the Visualisation page, Sample VS Sample; Bin VS Bin; Taxonomy comparison; PCA, and briefly describes each one.\n\n\n\nFigure: Home Page Example",
    "crumbs": [
      "Web Interface",
      "The Pankegg Web Page"
    ]
  },
  {
    "objectID": "web_interface.html#bin",
    "href": "web_interface.html#bin",
    "title": "The Pankegg Web Page",
    "section": "Bin",
    "text": "Bin\nThe Bin information page allows you to view and manage all bins in your project, organized by sample. At the top, you will find options to toggle the visibility of all bins or all taxonomy classifications with a single click.\nA panel provides advanced sorting, search, and filtering options: - Sort by Bin Name, Completeness, or Contamination. - Filter/search by sample name, bin name, or any taxonomic rank (Kingdom, Phylum, Class, Order, Family, Genus, Species). By default the search is on the sample name and bin name, for filtering by taxonomic rank select one or more before searching. - GTDB quality filtering can be enabled to display only the bins passing Genome Taxonomy DataBase selection criteria (completeness - 5*contamination &gt;50).\nEach bin is displayed in a table below its Sample Name, with columns for bin name, genus, species, completeness, and contamination.\nFor each bin, you can: - View associated maps (pathways) or KEGG orthologs with a single click. - Toggle taxonomy classification to expand or collapse the display of detailed taxonomic information. - Generate a quality plot for the bins in a sample.\nThese features make it easy to explore, filter, and compare bin quality and taxonomy across all your samples.\n\n\n\nFigure: Bin Information Page",
    "crumbs": [
      "Web Interface",
      "The Pankegg Web Page"
    ]
  },
  {
    "objectID": "web_interface.html#map",
    "href": "web_interface.html#map",
    "title": "The Pankegg Web Page",
    "section": "Map",
    "text": "Map\nThe Map information page gives you an overview of all metabolic pathways (KEGG maps) present in your dataset. Each pathway is listed with its map number, pathway name, and a completion percentage, which indicates the proportion of KEGG orthologs (KOs) found in your data relative to the total number of KOs for that pathway.\nAt the top of the page, you’ll find options to toggle all map details and export your current view. The legend explains how KOs are visualized:\n\nDark blue: KEGG ID officially present in the pathway\nLight blue: KEGG ID not officially present in the pathway\n\nNote: Not all KOs contribute equally to pathway function, so this value is only an approximate indicator.\n\nFiltering and Search\nYou can filter maps by sample, bin, pathway name, or KO identifier using the controls above the table. When filtering by bin or sample, the pathway completion percentage is recalculated to only include the KOs found within the selected bin(s) or sample(s), providing a focused view of its metabolic potential.\n\n\nViewing KO Details\nFor each pathway, you can toggle the display of all detected KO IDs. This expands the row to show the full list of associated KEGG orthologs, with coloring based on their official status in the pathway.\nIf you filter through KO IDS, the filtered KO will be highlighted in orange to differentiate it from all the other KO IDs.\n\n\n\nFigure: Map Information Page\n\n\n\n\nKEGG Info and Pathway Highlighting\nThe “KEGG info” button provides a direct link to the pathway’s entry on the KEGG website for further reference.\nThe “Highlight pathways” button opens a sidebar slider with all relevant KO IDs for the selected pathway. You can quickly copy these IDs and use them in the KEGG color tool to generate a custom view of the pathway map on the KEGG website. The KOs found in your current data will then be highlighted with color.\nThis combination of interactive filtering, KO details, and external links makes it easy to explore pathway presence, completion, and functional highlights across all your bins and samples.\n\n\n\nFigure: Map Information Page\n\n\nFor examples the (Carbon Metabolism pathway)[https://www.kegg.jp/pathway/map01200] for Sample 1 and 3 of the test data contains:\n\n\n\nFigure: Map Information Page",
    "crumbs": [
      "Web Interface",
      "The Pankegg Web Page"
    ]
  },
  {
    "objectID": "web_interface.html#kegg",
    "href": "web_interface.html#kegg",
    "title": "The Pankegg Web Page",
    "section": "KEGG",
    "text": "KEGG\nThe KEGG page provides an overview of all KEGG orthologs (KO IDs) detected in your dataset. The main table displays, for each KEGG ortholog (KO), its ID, KO name, and a full name/description. You can use the search bar at the top to filter the KEGG IDs.\nYou see the complete list of KOs across your entire project, with quick-access buttons to:\n\nView Bins: Show which bins contain the selected KO.\nView Maps: See which metabolic pathways (maps) include the selected KO.\nView Details: Expand to show detailed information for each bin and sample in which the KO is present, including associated GO terms and EggNOG annotations, if available.\nKEGG info: Open the corresponding KO entry on the KEGG website for more detailed biological context.\n\nWhen you apply filters on the Bin page, the KEGG Identifiers view will display only those KOs present in the selected bin, helping you focus on its functional profile.\nBeware that when viewing the details of KOs you can have multiple entries within the same bin as we parse the entire EggNOG annotation file and those files can be redundant.\nAll action buttons are designed for seamless exploration: filter bins and maps based on any KO of interest, or instantly jump to its external reference.\n\n\n\nFigure: KEGG Identifiers Page",
    "crumbs": [
      "Web Interface",
      "The Pankegg Web Page"
    ]
  },
  {
    "objectID": "web_interface.html#taxonomy",
    "href": "web_interface.html#taxonomy",
    "title": "The Pankegg Web Page",
    "section": "Taxonomy",
    "text": "Taxonomy\nThe Taxonomy page allows you to explore the taxonomic composition of your dataset at any rank (such as phylum, class, order, etc.) by simply selecting the desired taxonomic level from the dropdown menu at the top of the page. The resulting table lists all taxa detected at that rank, alongside the number of bins classified under each taxon.\nThe “unclassified” taxa are qualified in this table as their rank letter followed by 2 underscore (e.g. S__).\nFor each taxon, you have quick access buttons to:\n\nView only the bins classified as this taxon,\nSee the maps (metabolic pathways) found in those bins,\nBrowse the KEGG orthologs associated with this taxon.\n\nThis makes it easy to drill down into the taxonomic groups of interest and immediately access qualitative and pathway-related information for any selected group.\n\n\n\nFigure: Taxonomy Page",
    "crumbs": [
      "Web Interface",
      "The Pankegg Web Page"
    ]
  },
  {
    "objectID": "web_interface.html#sample-vs-sample",
    "href": "web_interface.html#sample-vs-sample",
    "title": "The Pankegg Web Page",
    "section": "Sample vs Sample",
    "text": "Sample vs Sample\nThe Sample vs Sample page enables detailed comparison between any two selected samples in your dataset.\nTo begin, choose two samples from the dropdown menus. The page then displays a suite of visualizations and tables to help you interpret differences and similarities:\n\nHeatmap:\nSelect a pathway category to visualize completion levels for each bin and each pathway in both samples. Pathway completion is shown as a heatmap—bins with more complete pathways are colored closer to red, while less complete pathways are closer to blue. You can also select multiple pathway categories. Each category is plotted individually and displayed underneath each other. Note: Completion is calculated as the proportion of orthologs detected in each bin out of the total number required for the pathway.\nScatterplot:\nThe bin quality scatterplot shows, for each sample, the completeness versus contamination for all bins, allowing quick assessment of bin quality distribution within and between samples.\nBin PCA:\nPCA (Principal Component Analysis) plots are generated for all bins in each sample, displaying the distribution of bins in reduced dimensionality space. This highlights similarities or differences in bin composition and functional potential. Note: This should only be used if there are sufficient amount of bins provided, a PCA on e.g. 3 bins has no value\nCommon Pathways:\nA table lists all detected pathways, with counts of shared and unique orthologs for each sample. You can filter pathways using the search bar. Below, a barplot provides a visual summary of these counts, making it easy to spot pathways enriched or unique to each sample.\n\nThese combined tools offer a comprehensive, multi-angle comparison of the functional and taxonomic profiles of your samples.\n\n\n\nFigure: Sample vs Sample Page",
    "crumbs": [
      "Web Interface",
      "The Pankegg Web Page"
    ]
  },
  {
    "objectID": "web_interface.html#bin-vs-bin",
    "href": "web_interface.html#bin-vs-bin",
    "title": "The Pankegg Web Page",
    "section": "Bin vs Bin",
    "text": "Bin vs Bin\nThe Bin vs Bin comparison page allows you to directly compare the metabolic potential of any two bins, whether they belong to the same sample or to different samples. After selecting two bins, the page displays a table listing all metabolic pathways, showing the count of orthologs detected in both bins, or uniquely in each bin. You can use the search bar to filter pathways of interest.\nBelow the table, a barplot visually summarizes these pathway counts, making it easy to identify pathways that are shared or unique to each bin.\nThis focused comparison makes it simple to explore functional similarities and differences between any two specific genome bins in your dataset.\n\n\n\nFigure: Bin vs Bin Page",
    "crumbs": [
      "Web Interface",
      "The Pankegg Web Page"
    ]
  },
  {
    "objectID": "web_interface.html#taxonomy-comparison",
    "href": "web_interface.html#taxonomy-comparison",
    "title": "The Pankegg Web Page",
    "section": "Taxonomy Comparison",
    "text": "Taxonomy Comparison\nThe Taxonomy Comparison page allows you to compare the taxonomic composition and metabolic potential of your samples at any chosen rank (such as phylum, class, or order). After selecting a rank, a barplot displays the percentage of bins belonging to each taxon across all samples. Note: The percentages are calculated as the number of bins for each taxon divided by the total number of bins in each sample.\nBelow, a heatmap visualizes the number of bins present at specific metabolic pathways for each taxon at the selected rank. Pathways (rows) are compared against taxonomic groups (columns), with color indicating the number of bins: red signifies higher number of bins, while blue indicates lower number of bins.\nThese combined visualizations help you to quickly assess both the taxonomic structure and functional diversity present in your data.\n\n\n\nFigure: Taxonomy Comparison Page",
    "crumbs": [
      "Web Interface",
      "The Pankegg Web Page"
    ]
  },
  {
    "objectID": "web_interface.html#pca",
    "href": "web_interface.html#pca",
    "title": "The Pankegg Web Page",
    "section": "PCA",
    "text": "PCA\nThe PCA page provides Principal Component Analysis (PCA) visualizations to help you explore the global structure and relationships in your dataset. You can choose to perform PCA:\n\nBased on KOs: The samples are differentiated by their KEGG Orthologs.\nBased on Maps: The samples are differentiated by their metabolic pathways (maps).\nBased on Taxonomy: The samples are differentiated by their taxonomic composition based on the specified rank (such as phylum, class, order, etc.).\n\nThe resulting PCA plot visualizes your samples in a reduced dimensional space, helping to highlight patterns, clusters, or differences driven by functional or taxonomic profiles. The explained variance for the principal components is displayed below the plot to indicate how much of the data’s variation is captured. Note: We recommend using this tool only if you have enough samples.\n\n\n\nFigure: PCA Page",
    "crumbs": [
      "Web Interface",
      "The Pankegg Web Page"
    ]
  }
]